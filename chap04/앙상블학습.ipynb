{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <지도학습>\n",
    "#     - 분류:4장 - Tree 계열 -> RF,GBM,XBRoot\n",
    "#               - SVM : vector 계열\n",
    "#     - 회귀:5장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd8bc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa79483b",
   "metadata": {},
   "source": [
    "# 머신러닝 실습 로드맵\n",
    "\n",
    "## 1. 결정 트리 (Decision Tree)\n",
    "- 1.1 결정 트리란?\n",
    "- 1.2 결정 트리의 주요 파라미터\n",
    "- 1.3 결정 트리 시각화: Graphviz [X]\n",
    "- 1.4 [실습] Scikit-Learn 을 이용한 결정 트리 구현\n",
    "- 1.5 제약 조건에 따른 모델 변화\n",
    "- 1.6 결정 트리와 과적합(Overfitting) \n",
    "- 1.7 [응용] 사용자 행동 인식 데이터 분류\n",
    "\n",
    "## 2. 앙상블 학습 (Ensemble Learning)\n",
    "- 2.1 앙상블 학습의 개념\n",
    "- 2.2 보팅 (Voting)\n",
    "- 2.3 배깅 (Bagging)과 랜덤 포레스트 (Random Forest) --> 랜포:오래됬지만 그나말 잘됨\n",
    "- 2.4 부스팅 (Boosting)\n",
    "\n",
    "## 3. 부스팅 알고리즘: GBM, XGBoost, LightGBM\n",
    "- 3.1 GBM (Gradient Boosting Machine)\n",
    "- 3.2 XGBoost (eXtra Gradient Boost)\n",
    "- 3.3 LightGBM\n",
    "\n",
    "## 4. 베이지안 최적화 기반 HyperOpt\n",
    "- 4.1 하이퍼파라미터 튜닝의 어려움  -> 진짜어렵다는걸 느껴야함[semi-project]\n",
    "- 4.2 베이지안 최적화와 HyperOpt ->베이지안 관련책 읽는게 좋음\n",
    "- 4.3 [실습] HyperOpt 를 이용한 XGBoost 하이퍼파라미터 튜닝\n",
    "\n",
    "## 5. 종합 실습 1: [산탄데르] 고객 만족 예측\n",
    "- 5.1 데이터 전처리\n",
    "- 5.2 XGBoost 모델 학습 및 평가\n",
    "- 5.3 HyperOpt 를 이용한 하이퍼파라미터 튜닝\n",
    "- 5.4 LightGBM 모델 학습 및 평가\n",
    "\n",
    "## 6. 종합 실습 2: 신용카드 사기 검출\n",
    "- 6.1 불균형 데이터셋의 이해\n",
    "- 6.2 데이터 전처리 및 모델 평가\n",
    "- 6.3 데이터 분포 변환 후 모델 성능 비교 -> 정규분포 안따름\n",
    "- 6.4 이상치 데이터 제거 후 모델 성능 비교\n",
    "- 6.5 SMOTE 오버 샘플링 적용 : 불균형 데이터  해결 기법\n",
    "- 6.6 최종 성능 요약\n",
    "\n",
    "## 7. 스태킹 앙상블 (Stacking Ensemble)\n",
    "- 7.1 스태킹의 개념\n",
    "- 7.2 [실습] 스태킹 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70696f7c",
   "metadata": {},
   "source": [
    "#### 1.결정 트리 : 스무고개\n",
    "- 트리기반의 분류규칙을 만드는 알고리즘\n",
    "- 1.질문할땐? -> 범위를 넓게, 이것저것 다 섞여 있는 질문(불순도 많은 질문)->불순도 적은 질문\n",
    "- 2.위에질문들을 다 기억해야함\n",
    "\n",
    "#### 1.2 결정트리\n",
    "- 데이터의 특징(feature)을 기반으로 질문을 던져가며 정답을 찾아가는 모델\n",
    "- 노드(Node):각 질문\n",
    "- '가지(Branch)'를 통해 최종 결론인 '잎 노드(Leaf Node)'에 도달\n",
    "- 밑이 있어야 위가 있다~~!! 아래규칙이 있어야 상위 규칙이 생긴다!\n",
    "- 루트노드(가장상위데이터) : 전체데이터로 만든다.\n",
    "- 리프노드: 더이상 나눌수 없는 노드(끝)\n",
    "- 단점)트리가 많으면 또 데이터 처리도 많많치 않음\n",
    "\n",
    "#### 1.2.1 불순도\n",
    "- 불순도 낮추게할려면: 각 영역의 데이터가 한 가지 종류로만 구성되도록 만드는 것\n",
    "- 리프노드 갈려면: 불순도=0 되야함\n",
    "\n",
    "#### 트리알고리즘 목표: 순도 높은 영역(leaf,Node) 만들기\n",
    "- 불순도 측정지표: 지니, 엔트로피\n",
    "- 측정지표로 불순도 감소 ---> 학습\n",
    "- [1]Gini Index :범위[(0~0.5)], 속도빠름 , 가장 큰 클래스를 고립시키는 형상\n",
    "- 학습했을때 쏠림현상 발생 가능  근데 문제생기면 '엔트로피'봐야함\n",
    "- [2]Entropy : 로그포함됨, 범위[0~1], 속도느림 , 균형 있는 분할하는 경향\n",
    "- ex) 불균형 데이터 나올때 엔트로피 다시 볼 필요있다.\n",
    "- 결론)[1]지니계수가 더 빠름(엔트로피는 로그 포함됨)\n",
    "- 로그쓰는 이유: 정보량 때문에 , 로그밑에2는 2bit\n",
    "- 지니계수쓰는이유: 속도때문에, 앤트로피랑은 차이없음\n",
    "- 이득= 정보량(분산이 크다)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220514ff",
   "metadata": {},
   "source": [
    "| 파라미터 이름         | 설명                                                                 | 값 예시 또는 기본값         |\n",
    "|----------------------|----------------------------------------------------------------------|-----------------------------|\n",
    "| `criterion`          | 분할 품질을 측정하는 기준                                            | `'gini'`, `'entropy'` (기본값: `'gini'`) |\n",
    "| `splitter`           | 각 노드에서 분할을 선택하는 전략                                     | `'best'`, `'random'` (기본값: `'best'`) |\n",
    "| `max_depth`          | 트리의 최대 깊이. 작게 설정하면 과적합 방지에 도움됨                | 정수 (예: `None`, `5`, `10`) |\n",
    "| `min_samples_split`  | 노드를 분할하기 위한 최소 샘플 수. 과적합 제어에 사용됨              | 정수 또는 실수 (예: `2`, `0.05`) |\n",
    "| `min_samples_leaf`   | 리프 노드가 되기 위한 최소 샘플 수. 과적합 제어에 사용됨             | 정수 또는 실수 (예: `1`, `0.01`) |\n",
    "| `max_features`       | 최적의 분할을 찾을 때 고려할 피처의 최대 개수                        | `'auto'`, `'sqrt'`, `'log2'`, 정수, 실수 |\n",
    "| `max_leaf_nodes`     | 리프 노드의 최대 개수. 트리 복잡도를 제한할 수 있음                 | 정수 (예: `None`, `10`, `100`) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377444f5",
   "metadata": {},
   "source": [
    "#### 과적합: 학습된 데이터로만 학습되서(?) new data에 대해서는 성능 떨어짐\n",
    "#### 과적합방지위해서 항상 model 학습은 new data로만 , 학습데이터로는 하면 안됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e0d09",
   "metadata": {},
   "source": [
    "### 알림: DEcision TREE안쓰지만 모든 트리가 공통적으로 특징 있기에 배우는거임\n",
    "### 결론) Decision Tree안쓴다~~~~~~~~~~~~~~~~~~~~~~~!!!!!!!!!!!\n",
    "\n",
    "#### [1]criterion,splitter ->디폴트 처리해도 OK\n",
    "#### [2]max_depth~max_leaf_nodes : 과적합 방지\n",
    "#### [3]max_dept : 복잡한넘 -> 단순 (단 ,tree 는 끝까지 나눠서 항상 과적합 나옴)\n",
    "- 중간에 가지를 확 쳐버리는것\n",
    "- <참고> 과적합이 나쁜게 아니라 제어만 잘 하면 된다~\n",
    "#### [4]min_samples_split:조금더 상세하게 제어, 해봐야지 아는것(이게 어려움)\n",
    "#### [5]min_samples_leaf:더이상 나누지 않음 / leaf: 잎새라서 여기가 마지막\n",
    "- split과 비슷한 개념,그래서 두개다 같이 쓰지 않음\n",
    "#### [6]max_features : <중요> : 컬럼의 최대갯수 뽑을 수 있음\n",
    "- 피처 =컬럼 , 10개 칼럼에서 4개 칼럼만 가져올수 있는거~~, 어떤 컬럼이 중요한지 안한지 알 수 있음\n",
    "- 10개다 가져오면 과적합 될 수 있으니까~~\n",
    "- sqrt:제곱근, 많이 사용함~~\n",
    "- log 언제사용? feature 갯수 너무 많을때\n",
    "- 정수 : 갯수 , 실수 : Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292670d",
   "metadata": {},
   "source": [
    "#### 1.3결정트리 시각화: Graphiz\n",
    "#### 시각화하는 이유: 과적합이 어떻게 보여지는지 확인할려고~, 이해 편의성 위해서\n",
    "- 과적합 제어\n",
    "- feature 중요도 파악\n",
    "\n",
    "#### 트리 시각화 라이브러리\n",
    "#### [1]plot_tree() : matplotlib 사용\n",
    "#### [2]dtreeviz() : 설치필요,분포표시 -> 해석쉬움\n",
    "#### [3]Graphivz():\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc770e9",
   "metadata": {},
   "source": [
    "#### 부스팅 알고리즘: GBIM, XGBoost,LightGBM / 병령처리 X ,오류에 가중치 줌\n",
    "\n",
    "#### GBM:신경망에 나오는 부스팅이랑, 머신러닝에 나오는 부스팅이랑 다르넹~\n",
    "#### XGBoost: 성능좋음, 대부분은 성능좋은걸 좋아해서 XGboost 사용 많음\n",
    "#### lightGBM:속도빠름\n",
    "#### 부스팅: 기울기(Gradient)사용함, 이전 모델의 예측오류(잔차, residual)을 다음모델이 학습하도록 오류줄여나가기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad0c1d",
   "metadata": {},
   "source": [
    "#### 딥러닝\n",
    "- 데이터 양이 많아야함\n",
    "- 양이 적을땐 ---> XGBoost 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09289734",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
