{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <지도학습>\n",
    "#     - 분류:4장 - Tree 계열 -> RF,GBM,XBRoot\n",
    "#               - SVM : vector 계열\n",
    "#     - 회귀:5장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79483b",
   "metadata": {},
   "source": [
    "# 머신러닝 실습 로드맵\n",
    "\n",
    "## 1. 결정 트리 (Decision Tree)\n",
    "- 1.1 결정 트리란?\n",
    "- 1.2 결정 트리의 주요 파라미터\n",
    "- 1.3 결정 트리 시각화: Graphviz [X]\n",
    "- 1.4 [실습] Scikit-Learn 을 이용한 결정 트리 구현\n",
    "- 1.5 제약 조건에 따른 모델 변화\n",
    "- 1.6 결정 트리와 과적합(Overfitting) \n",
    "- 1.7 [응용] 사용자 행동 인식 데이터 분류\n",
    "\n",
    "## 2. 앙상블 학습 (Ensemble Learning)\n",
    "- 2.1 앙상블 학습의 개념\n",
    "- 2.2 보팅 (Voting)\n",
    "- 2.3 배깅 (Bagging)과 랜덤 포레스트 (Random Forest) --> 랜포:오래됬지만 그나말 잘됨\n",
    "- 2.4 부스팅 (Boosting)\n",
    "\n",
    "## 3. 부스팅 알고리즘: GBM, XGBoost, LightGBM\n",
    "- 3.1 GBM (Gradient Boosting Machine)\n",
    "- 3.2 XGBoost (eXtra Gradient Boost)\n",
    "- 3.3 LightGBM\n",
    "\n",
    "## 4. 베이지안 최적화 기반 HyperOpt\n",
    "- 4.1 하이퍼파라미터 튜닝의 어려움  -> 진짜어렵다는걸 느껴야함[semi-project]\n",
    "- 4.2 베이지안 최적화와 HyperOpt ->베이지안 관련책 읽는게 좋음\n",
    "- 4.3 [실습] HyperOpt 를 이용한 XGBoost 하이퍼파라미터 튜닝\n",
    "\n",
    "## 5. 종합 실습 1: [산탄데르] 고객 만족 예측\n",
    "- 5.1 데이터 전처리\n",
    "- 5.2 XGBoost 모델 학습 및 평가\n",
    "- 5.3 HyperOpt 를 이용한 하이퍼파라미터 튜닝\n",
    "- 5.4 LightGBM 모델 학습 및 평가\n",
    "\n",
    "## 6. 종합 실습 2: 신용카드 사기 검출\n",
    "- 6.1 불균형 데이터셋의 이해\n",
    "- 6.2 데이터 전처리 및 모델 평가\n",
    "- 6.3 데이터 분포 변환 후 모델 성능 비교 -> 정규분포 안따름\n",
    "- 6.4 이상치 데이터 제거 후 모델 성능 비교\n",
    "- 6.5 SMOTE 오버 샘플링 적용 : 불균형 데이터  해결 기법\n",
    "- 6.6 최종 성능 요약\n",
    "\n",
    "## 7. 스태킹 앙상블 (Stacking Ensemble)\n",
    "- 7.1 스태킹의 개념\n",
    "- 7.2 [실습] 스태킹 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70696f7c",
   "metadata": {},
   "source": [
    "#### 1.결정 트리 : 스무고개\n",
    "- 트리기반의 분류규칙을 만드는 알고리즘\n",
    "- 1.질문할땐? -> 범위를 넓게, 이것저것 다 섞여 있는 질문(불순도 많은 질문)->불순도 적은 질문\n",
    "- 2.위에질문들을 다 기억해야함\n",
    "\n",
    "#### 1.2 결정트리\n",
    "- 데이터의 특징(feature)을 기반으로 질문을 던져가며 정답을 찾아가는 모델\n",
    "- 노드(Node):각 질문\n",
    "- '가지(Branch)'를 통해 최종 결론인 '잎 노드(Leaf Node)'에 도달\n",
    "- 밑이 있어야 위가 있다~~!! 아래규칙이 있어야 상위 규칙이 생긴다!\n",
    "- 루트노드(가장상위데이터) : 전체데이터로 만든다.\n",
    "- 리프노드: 더이상 나눌수 없는 노드(끝)\n",
    "- 단점)트리가 많으면 또 데이터 처리도 많많치 않음\n",
    "\n",
    "#### 1.2.1 불순도\n",
    "- 불순도 낮추게할려면: 각 영역의 데이터가 한 가지 종류로만 구성되도록 만드는 것\n",
    "- 리프노드 갈려면: 불순도=0 되야함\n",
    "\n",
    "#### 트리알고리즘 목표: 순도 높은 영역(leaf,Node) 만들기\n",
    "- 불순도 측정지표: 지니, 엔트로피\n",
    "- 측정지표로 불순도 감소 ---> 학습\n",
    "- [1]Gini Index :범위[(0~0.5)], 속도빠름 , 가장 큰 클래스를 고립시키는 형상\n",
    "- 학습했을때 쏠림현상 발생 가능  근데 문제생기면 '엔트로피'봐야함\n",
    "- [2]Entropy : 로그포함됨, 범위[0~1], 속도느림 , 균형 있는 분할하는 경향\n",
    "- ex) 불균형 데이터 나올때 엔트로피 다시 볼 필요있다.\n",
    "- 결론)[1]지니계수가 더 빠름(엔트로피는 로그 포함됨)\n",
    "- 로그쓰는 이유: 정보량 때문에 , 로그밑에2는 2bit\n",
    "\n",
    "#### 속도:성능에 문제는 안되지만,(0~0.5)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220514ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
