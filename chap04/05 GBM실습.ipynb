{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f74749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np \n",
    "# import time\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import warnings\n",
    "# import human_dataset as hd\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edef2f2",
   "metadata": {},
   "source": [
    "- 부스팅 알고리즘: GBIM, XGBoost,LightGBM / 병령처리 X ,오류에 가중치 줌\n",
    "\n",
    "- GBM:신경망에 나오는 부스팅이랑, 머신러닝에 나오는 부스팅이랑 다르넹~\n",
    "- XGBoost: 성능좋음, 대부분은 성능좋은걸 좋아해서 XGboost 사용 많음\n",
    "- lightGBM:속도빠름\n",
    "- 부스팅: 기울기(Gradient)사용함, 이전 모델의 예측오류(잔차, residual)을 다음모델이 학습하도록 오류줄여나가기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 처리\n",
    "\n",
    "# X_train,X_test,y_train, y_test = hd.get_human_dataset()\n",
    "\n",
    "# 학습 시간 측정, 시작 시간저장\n",
    "# start_time = time.time()#현재 날짜 시간 반환 , fit하기전 시작시간이 된다\n",
    "# gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "# gb_clf.fit(X_train,y_train)\n",
    "# pred = gb_clf.predict(X_test)\n",
    "# gb_acc= accuracy_score(y_test, pred)\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f'정확도:{gb_acc:.4f}')\n",
    "# print(f'수행시간:{end_time - start_time}') # 학습이 끝난시간\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54851020",
   "metadata": {},
   "source": [
    "- loss: 경사 하강법에 사용할 비용 함수. => 오차구하는함수(암기)\n",
    "- learning_rate: 학습률. 각 트리가 예측 결과에 기여하는 정도를 조절합니다. 너무 크면 과적합(반드시 그런건 X,발산도 가능), 너무 작으면 학습이 느려질,개발자가 줘야할 값\n",
    "수 있습니다. 보통 0.0001 ~ 0.1 사이의 값을 사용합니다,적당값 부여해야함\n",
    "- n_estimators: 생성할 약한 학습기(트리)의 개수. 많을수록 성능이 좋아질 수 보장 X,과적합은 많아짐, 시간도 오래 걸리고 과적합의 위험이\n",
    "있습니다.\n",
    "- subsample: 각 트리를 학습할 때 사용할 데이터 샘플의 비율. (배깅과 유사)\n",
    "- learning_rate 와 n_estimators 는 서로 트레이드오프 관계에 있습니다.\n",
    "    - <장담 X, ~할수 있다>\n",
    "    - 학습률 작아지면 -> 학습시간 증가(왜? 꼼꼼이 해서) -> 성능증가-> 과적합 가능성 증가\n",
    "    - 학습률 높아지면 -> 학습시간 감소 -> 과소적합->일반화성능 무조건 감소\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c255a5f",
   "metadata": {},
   "source": [
    "- XGBoost(eXtra Gradient Boost)\n",
    "- cpu/Gpu(병렬처리)\n",
    "- 규제(Regularization) --> 일반화성능, 뭉겐다.\n",
    "- XGbookst 쓰는 이유: GBM은 규제가 안되서\n",
    "- 결론)XGboost 성능은 규제다 ,일반화성능 이 더 좋다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92f9d0",
   "metadata": {},
   "source": [
    "- conda-forge(채널), repository\n",
    "- 가장 최신 정보들어가 있음\n",
    "- 콘다의 특정 라이브러리 설치\n",
    "- 기여자(별도의 C언어도 패키징 한다)\n",
    "- conda install -c anaconda py-xgboost (콘다 사용시)\n",
    "- Xgboost 사용위해선 c언어 컬라일러 빌러, Driver,cuda tookit 다 사용해야함=> 어차피 실행은 C언어로 됨\n",
    "\n",
    "- 딥러닝 가서 GPU설치할려면 pytorch사용하면 됨(편함)\n",
    "- 딥러닝가서 파이썬기반, 사이킹기반 라이브러리인지 판단후 코딩해야함\n",
    "- => 콘다포지에 가면 한번에 처리하는 넘 이 있다.\n",
    "\n",
    "- Conda -> anaconda, conda-forge 선택가능\n",
    "- pip -> pip만 가능\n",
    "\n",
    "- <pip>\n",
    "- c:\\aaa>pip venu\n",
    "-  --> venu 디렉토리 설치\n",
    "- 다른환경과 영향 가지 않음 , 개별관리\n",
    "- uv(가상환경 관리, 패키지 관리 :콘다와같은기능, 속도10배이상) 사용\n",
    "- 결론)pip로 설치했으면 pip로 계속 가라, 섞으면 안됨\n",
    "- 단) 어쩔수 없는 경우엔 conda_forge 채널(최신인 경우)\n",
    "\n",
    "- <conda> : 조심해야함\n",
    "- conda create 이름\n",
    "- 통합관리 \n",
    "- 문제생기면 다시 설치애햐암, cache 처리\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
